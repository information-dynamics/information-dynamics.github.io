---
layout: post
title:  "How to measure high-order effects with the O-information"
date:   2019-06-15 19:54:09 +0100
categories: complexity information
description: 'The Lempel-Ziv complexity (LZ) is a popular tool to quantify the uncertainty
contained in time series data. In particular, LZ measures how "diverse"
are the patterns that are present in a particular signal.'
---

We are the first generation living in the era of the so-called "big data". For first time we have at disposal aboundant data, with which we can deepen our understanding 
systems of interest, in particular of complex systems. Traditionally, mathematics and physics used to work based on simple analytical models, which thoughtfully analysed in order to forge intuition and understanding of various phenomena. In contrast, nowadays we can explore phenomena using only data without relying in abstract models. While most people associate model-free data analysis with Machine Learning, there is a big opportunity in data driven model-free approaches to study complex systems. In effect, plentiful data is nowadays available about e.g. the orchestrated activity of multiple brain areas, the relationship between various econometric indices, the interactions between different genes, and a pletora of other fascinating systems. 

While conventional machine learning works under the motto of non-interpretable predictive power, Complexity Science have orthogonal aims. One of the key questions in Complexity Theory is to understand what allows some systems to be "more than the sum of their parts". Clearly, this synergy is not related to the nature of the parts, but is somehow contained in the structure of their interdependencies. However, quantifying the synergy related to different interdependency structures is far from straightforward.


Synergistic relationships and other high-order interactions have been studied for many years in the literature of theoretical neuroscience. For example, studies on how neurons encode information have found that neurons can carry redundant, complementary or synergistic information -- the latter corresponding to neurons that are uninformative individually but informative when considered together [[schneidman2003synergy](#schneidman2003synergy),[latham2005synergy](#latham2005synergy)]. Additionally, studies on retina cells suggest that high-order interactions are needed to fit the neurons firing patterns in response to natural images (pictures of landspaces, etc), while pairwise interactions suffice for neurons responding to less structured stimuli (e.g. white noise) [[ganmor2011sparse](#ganmor2011sparse)]. Finally, neuroimaging analyses have pointed out the compatibility of local differentiation and global integration of different brain areas, and suggested that this could be a key capability required for enabling high brain functions [[tononi1998complexity](#tononi1998complexity),[[balduzzi2008integrated](#balduzzi2008integrated)]. 


A promising approach for addressing high-order interdependencies is *partial information decomposition* (PID), which distinguishes different "types" of information that 
multiple predictors can convey about a given target variable [[williams2010nonnegative](#williams2010nonnegative),[[griffith2014quantifying](#griffith2014quantifying)],[[wibral2017partial](#wibral2017partial)]. In this framework, *statistical synergies* are structures (or relationships) that exist in the joint statistics 
but cannot be found in low-order marginals, being this rooted in the elementary fact that
variables can be pairwise independent while being globally correlated. 
Unfortunately, the adoption of PID has been hindered by the lack of agreement on 
how to compute the components of the decomposition, despite numerous recent efforts [[barrett2015exploration](#barrett2015exploration)],[[ince2017measuring](#ince2017measuring)],[[james2018unique](#james2018unique)],[[finn2018pointwise],(finn2018pointwise)]. 
Furthermore, although practical applications of PID have been reported [[tax2017partial](#tax2017partial)],[[wibral2017quantifying](#wibral2017quantifying)],[[faes2017multiscale](#faes2017multiscale)], the applicability of the framework is greatly restricted by the rapid growth of the number of terms for large systems.


The crux of multivariate interdependencies is that information-theoretic descriptions of such phenomena are not straighforward, as extensions of Shannon's classical results to general multivariate settings have proven elusive [[el2011network](#el2011network)].
The most well-established multivariate extensions of Shannon's mutual information are
the *total correlation* [[watanabe1960information](#watanabe1960information)] and the
*dual total correlation* [[han1975linear](#han1975linear)], which provide suitable
metrics of overall correlation strength. Their values, however, differ
in ways that are hard to understand even gaining the adjective
of "enigmatic" among scholars [[james2011anatomy](#james2011anatomy)],[[vijayaraghavan2017anatomy](#vijayaraghavan2017anatomy)].
Other popular extension of the mutual information is the
*interaction information* [[mcgill1954multivariate](#mcgill1954multivariate)] which is a signed measure obtained by applying the inclusion-exclusion principle to the Shannon entropy 
[[Ting1962](#Ting1962)],[[yeung1991](#yeung1991)]. Although this metric provides insighful results when applied to three variables, its is not easily interpretable when applied to larger groups [[williams2010nonnegative](#williams2010nonnegative)].



This paper proposes to study multivariate interdependency via two dual persectives: 
as *shared randomness* and as *collective constraints*. This disctinction might not
have been stressed in the past because most studies focus on bivariate
interactions between two sets of variables, for which these two effects are equivalent and equal to the mutual information. 
However, for interactions involving three or more variables these perspectives differ. 

This setup leads to the *O-information*, which -- following Occam's razor -- points out which of these perspectives provides a more parsimonious description of the system. 
The O-information is found to coincide with the interaction information for the case of three variables, while providing a more meaningful extension for larger system sizes. 


We show how the O-information captures the dominant characteristic of
multivariate interdependency, distinguishing redundancy-dominated scenarios
where three or more variables have copies of the same information, and
synergy-dominated systems characterised by high-order patterns that cannot be
traced from low-order marginals. In contrast with existing quantities that
require a division between predictors and target variables, the O-information
is -- to the best of our knowledge -- the first symmetric quantity that can
give account of intrinsic statistical synergy in systems of more than three
parts. Moreover, as the computational complexity of the O-information scales
gracefully with system size, our framework provides a scalable approach for
applying PID principles to large systems, suitable for practical data analysis.
